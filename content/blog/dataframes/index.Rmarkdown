---
title: "Data frame wars: pandas vs. polars vs duckdb"
excerpt: "A comparison of pandas, polars and duckdb from the perspective of a dplyr user."
slug: "dataframes"
author: "Paul Simmering"
date: "2021-12-20"
categories: ["R", "Python"]
tags: ["Performance"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

I'm a long time R user and lately I've seen more and more signals that it's worth investing into learning more Python. I use it for NLP with [spaCy](https://spacy.io) and to build functions on [AWS Lambda](https://aws.amazon.com/lambda/features/) (though the recent [lambdr](https://mdneuzerling.com/post/serverless-on-demand-parametrised-r-markdown-reports-with-aws-lambda/) package gave R some great tools there too). Further, there are many more data API libraries available for Python than for R.

Michael Chow, developer of [siuba](https://github.com/machow/siuba), a Python port of dplyr on top of pandas [wrote](https://mchow.com/posts/pandas-has-a-hard-job/):

> It seems like there’s been a lot of frustration surfacing on twitter lately from people coming from R—especially if they’ve used dplyr and ggplot—towards pandas and matplotlib. I can relate. I’m developing a port of dplyr to python. But in the end, it’s probably helpful to view these libraries as foundational to a lot of other, higher-level libraries (some of which will hopefully get things right for you!).

And that summarizes how I feel about it: The syntax of pandas feels inferior to dplyr and I wish I could have the same fluency as with dplyr. The higher-level libraries he mentions come with a problem though: There's no accepted standard. So investing the time to learn one of them may be futile as the next project one is working on could be with other developers who are fluent in a different one.

## The contenders

Python has a larger package ecosystem and ways of doing things than R, which seems more centralized thanks to CRAN. Adopting Python means making many choices on which libraries to invest time into learning. The first thing I needed was a way to manipulate data frames. I've narrowed it down to 3 choices:

1. [Pandas](https://pandas.pydata.org): The most commonly used library and the one with the most tutorials and Stack Overflow answers available.
2. [siuba](https://github.com/machow/siuba): A port of dplyr to Python, built on top of pandas
3. [Polars](https://www.pola.rs): The fastest library available. It's a new library that doesn't have nearly as many users or help available. But according to the [H2Oai ops benchmark](https://h2oai.github.io/db-benchmark/), it runs 3-10x faster than Pandas.
4. [Duckdb](https://www.pola.rs): Use an in-memory OLAP database instead of a dataframe. I know SQL, so this is the easiest one to pick up.

There are far more options and this is my shortlist. I have excluded the other options in the benchmark for these reasons:

- Slower than polars (dask, Arrow, Modin)
- Not mature enough, as shown by lower activity on Github (pydatatable)
- Requires other software or a server (ClickHouse)
- Not in Python (DataFrames.jl)
- Meant for GPU only (cuDF)
- Wrappers like [ibis](https://ibis-project.org/docs/index.html) that delegate computation to pandas or a server running SQL. Technically siuba is also in this group, but as I'm coming from dplyr I had to include it.

The benchmark provides a comparison of performance, but another important factor is popularity and maturity. A more mature library has a more stable API, better test coverage and there is more help available online, such as on StackOverflow.

```{r github_stars}
library(ggplot2)
libs <- data.frame(
  library = c("pandas", "siuba", "polars", "duckdb", "dplyr", "data.table"),
  language = c("Python", "Python", "Python", "SQL", "R", "R"),
  stars = c(32100, 732, 3900, 4100, 3900, 2900)
)

ggplot(libs, aes(x = reorder(library, -stars), y = stars, fill = language)) +
  geom_col() + 
  labs(
    title = "Pandas is by far the most popular choice",
    subtitle = "Comparison of Github stars on 2021-12-25",
    fill = "Language",
    x = "Library",
    y = "Github stars"
  )
```

Github stars are not a perfect proxy. For instance, dplyr is much more mature than its star count suggests. Comparing the completen and completeness of the documentation of dplyr and polars reveals that it's a night and day difference.

My reference is my current use of [dplyr](https://dplyr.tidyverse.org) in R. When I need more performance, I use [tidytable](https://github.com/markfairbanks/tidytable) to get the speed of data.table with the grammar of dplyr. I also use [dbplyr](https://dbplyr.tidyverse.org) a lot, which translates dplyr to SQL. It's composeable, which actually makes it superior to SQL for me in most use cases.

With that out of the way, here's a heavily biased comparison of the four Python packages.

I'm speaking of my personal opinion of these packages given my own background - not a general comparison. I'll compare the three contenders by running a data transformation pipeline involving import from CSV, mutate, filter, sort, join, group by and summarise. I'll use the nycflights13 dataset, which some readers may know from Hadley Wickham's [R for Data Science](https://r4ds.had.co.nz/transform.html).

## dplyr: Reference in R

Let's start off the comparison with a reference implementation with my current default, dplyr. The dataset is available as a package, so I skip the CSV import step here.

```{r r_pkg_loading}
suppressMessages(library(dplyr))
library(nycflights13)
library(reactable)

# Take a look at the tables
reactable(head(flights, 10))
reactable(head(airlines, 10))
```

The `flights` tables has `r nrow(flights)` rows, one for each flight of an airplane. The `airlines` table has `r nrow(airlines)` rows, one for each airline mapping the full name of the company to a shortcode.

Let's find the airline with the highest arrival delays in January 2013. Some values in `arr_delay` are negative, indicating that the flight was faster than expected. I replace these values with 0 because I don't want them to cancel out delays of other flights. I join to the airlines table to get the full names of the airlines.

```{r dplyr_query}
flights |>
  filter(year == 2013, month == 1, !is.na(arr_delay)) |> 
  mutate(arr_delay = replace(arr_delay, arr_delay < 0, 0)) |>
  left_join(airlines, by = "carrier") |>
  group_by(airline = name) |>
  summarise(flights = n(), mean_delay = mean(arr_delay)) %>% 
  arrange(desc(mean_delay))
```

I export two tables from the dataset to CSV to make it available for Python packages.

```{r to_csv}
# Use data.table::fwrite instead of write.csv because it's faster
data.table::fwrite(flights, "flights.csv", row.names = FALSE)
data.table::fwrite(airlines, "airlines.csv", row.names = FALSE)
```

## Pandas: Most popular

The syntax is inspired by base R, which is a good thing.

```{python pandas_import}
import pandas as pd

# Import from CSV
flights_pd = pd.read_csv("flights.csv")
airlines_pd = pd.read_csv("airlines.csv")
```

`pandas.read_csv` read the header and conveniently inferred the column types.

```{python pandas_query}
(flights_pd
  .query("year == 2013 & month == 1 & arr_delay.notnull()")
  .assign(arr_delay = flights_pd.arr_delay.clip(lower = 0))
  .merge(airlines_pd, how = "left", on = "carrier")
  .rename(columns = {"name": "airline"})
  .groupby("airline")
  .agg(flights = ("airline", "count"), mean_delay = ("arr_delay", "mean"))
  .sort_values(by = "mean_delay", ascending = False))
```

Rows with missing values for `arr_delay` are dropped implicitly in the `agg` step.

Pandas uses a row index, which is basically a special column. Base R also has this with row names, though the tidyverse and tibbles have largely removed them from common use.

Pandas has the widest API, offering hundreds of functions for every conceivable manipulation.

## siuba: dplyr in Python

Data import to siuba is delegated to pandas.

```{python siuba_import}
from siuba import *
from siuba.dply.vector import *

# Import from CSV
flights_si = pd.read_csv("flights.csv")
airlines_si = pd.read_csv("airlines.csv")
```

```{python siuba_query}
(
  flights_si
    >> filter(
      _.year == 2013, 
      _.month == 1,
      _.arr_delay.notnull()
    )
    >> mutate(arr_delay = _.arr_delay.clip(lower = 0))
    >> left_join(_, airlines_si, on = "carrier")
    >> rename(airline = _.name)
    >> group_by(_.airline)
    >> summarize(
      flights = _.airline.count(),
      mean_delay = _.arr_delay.mean()
    )
    >> arrange(-_.mean_delay)
)
```

siuba is not sold for production use (from the [docs](https://siuba.readthedocs.io/en/latest/)):

> Siuba is a library for quick, scrappy data analysis in Python. It is a port of dplyr, tidyr, and other R Tidyverse libraries.

## Polars: Fastest

Polars is written in Rust and also offers a Python API. It comes in two flavors: eager and lazy. Lazy evaluation is similar to how dbplyr and dtplyer work: until asked, nothing is evaluated. This enables performance gains by reordering the commands being executed. But it's a little less convenient for interactive analysis. I'll use the eager API here.

```{python polars_import}
import polars as pl

# Import from CSV
flights_pl = pl.read_csv("flights.csv")
airlines_pl = pl.read_csv("airlines.csv")
```

The API is leaner than pandas, requiring to memorize fewer functions and patterns. Though this can also be seen as less feature-complete. Pandas, for example has a dedicated `clip` function.

```{python polars_query}
(flights_pl
  .filter((pl.col("year") == 2013) & (pl.col("month") == 1))
  .drop_nulls("arr_delay")
  .join(airlines_pl, on = "carrier", how = "left")
  .with_columns(
    [
      pl.when(pl.col("arr_delay") > 0)
        .then(pl.col("arr_delay"))
        .otherwise(0)
        .alias("arr_delay"),
      pl.col("name").alias("airline")
    ]
  )
  .groupby("airline")
  .agg(
    [
      pl.count("airline").alias("flights"),
      pl.mean("arr_delay").alias("mean_delay")
    ]
  )
  .sort("mean_delay", reverse = True)
)
```

There isn't nearly as much help available for problems with polars as for with pandas. Often, I had to use trial and error based on the documentation. While the documentation is good, it can't answer every question.

A comparison of polars and pandas is available in the [polars documentation](https://pola-rs.github.io/polars-book/user-guide/coming_from_pandas.html?highlight=assign#column-assignment). A notable difference is that polars doesn't have a concept of indexes, just like tibbles in R.

## DuckDB: Highly compatible and easy for SQL users

```{python duckdb_import}
import duckdb

con = duckdb.connect(database = ':memory:')

# Import from CSV
con.execute(
  "CREATE TABLE 'flights' AS "
  "SELECT * FROM read_csv_auto('flights.csv', header = True);"
  "CREATE TABLE 'airlines' AS "
  "SELECT * FROM read_csv_auto('airlines.csv', header = True);"
)
```

```{python duckdb_query}
con.execute(
  "WITH flights_clipped AS ( "
  "SELECT carrier, CASE WHEN arr_delay > 0 THEN arr_delay ELSE 0 END AS arr_delay "
  "FROM flights "
  "WHERE year = 2013 AND month = 1 AND arr_delay IS NOT NULL"
  ")"
  "SELECT name AS airline, COUNT(*) AS flights, AVG(arr_delay) AS mean_delay "
  "FROM flights_clipped "
  "LEFT JOIN airlines ON flights_clipped.carrier = airlines.carrier "
  "GROUP BY name "
  "ORDER BY mean_delay DESC "
).fetchdf()
```

The performance is closer to polars than to pandas.

A big plus is the ability to handle larger than memory data.

DuckDB can also operate directly on a pandas dataframe.

The code is portable to R, C, C++, Java and other programming languages the duckdb has [APIs](https://duckdb.org/docs/api/overview). It's also portable when the logic is taken to a DB like [Postgres](https://www.postgresql.org), or [Snowflake](https://www.snowflake.com/), or is ported to an ETL framework like [DBT](https://github.com/dbt-labs/dbt-core).

This stands in contrast to polars and pandas code, which has to be rewritten from scratch. It also means that the skill gained in manipulating data translates well to other situations - SQL has been around for more than 40 years. Something that can't be said about any Python library. Learning SQL is future-proofing ones career.

While these are big plusses, duckdb isn't as convenient as Polars and Pandas for interactive data exploration. The SQL isn't as composeable. 

Composing SQL queries requires many common table expressions (CTEs, `WITH x AS (SELECT ...)`).

Plus, writing strings rather than actual Python is awkward and many editors don't provide syntax highlighting within the strings (Jetbrains editors like [PyCharm](https://www.jetbrains.com/pycharm/) and [DataSpell](https://www.jetbrains.com/dataspell/) do).

SQL is less expressive than Python, especially when the names of output columns are unknown. 

It lacks shorthands. It's also harder to write custom functions in SQL. With pandas and polars, custom operations are just one lambda away.

Using duckdb without pandas doesn't seem feasible for exploratory data analysis, because graphing packages like seaborn and plotly expect a pandas data frame or similar as an input.

Speed of iteration is critical: the faster one can iterate, the more hypotheses about the data can be tested.

## Conclusion

It's not a clear-cut choice. Each seems more useful in it's own arena.

None of the three options offer a syntax that is as convenient for interactive analysis as dplyr. Polars is the closest to it, but dplyr still has an edge with [tidy evaluation](https://www.tidyverse.org/blog/2019/06/rlang-0-4-0/#a-simpler-interpolation-pattern-with), letting users refer to columns in a data frame by their names (`colname`) rather than as strings `"colname"`or constructs like `pl.col("colname")`. While this is nice for quickly writing code, I've also seen it be confusing for newbies to R that mix it up with base R's syntax. It's also harder to program with, where it's necessary to use operators like `{{ }}` and `:=`.

Personally, I'll leverage my existing knowledge and rely on SQL and an OLAP database (such as Snowflake) to do the heavy lifting. For steps that are better done locally, I'll use pandas for maximum compatibility. The syntax isn't my favorite, but there's so much online help available that StackOverflow has the answer for almost any problem. Github Copilot also deserves a mention for making it easier to pick up.

Most data science work happens in a team. Choosing a library that all team members are familiar with is critical for collaboration. That is typically SQL, pandas or dplyr. The performance gains from using a less common library like polars have to be weighed against the effort spent learning the syntax as well as the increased likelihood of bugs, when beginners write in a new syntax.

Related articles:

- [Polars: the fastest DataFrame library you've never heard of](https://www.analyticsvidhya.com/blog/2021/06/polars-the-fastest-dataframe-library-youve-never-heard-of/)
- [What would it take to recreate dplyr in python?](https://mchow.com/posts/2020-02-11-dplyr-in-python/)
- [Pandas has a hard job (and does it well)](https://mchow.com/posts/pandas-has-a-hard-job/)
- [dplyr in Python? First impressions of the siuba module](https://bensstats.wordpress.com/2021/09/14/pythonmusings-6-dplyr-in-python-first-impressions-of-the-siuba-小巴-module/)

Photo by <a href="https://unsplash.com/@hharritt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Hunter Harritt</a> on <a href="https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>
